{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def initialize_reddit():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id = os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        password= os.getenv(\"REDDIT_PASSWORD\"),\n",
    "        user_agent = os.getenv(\"REDDIT_USER_AGENT\"),\n",
    "        username = os.getenv(\"REDDIT_USERNAME\"),\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_subreddit(reddit, subreddit_name, search_keywords, post_limit=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Scrape posts and comments from a specific subreddit based on keywords.\n",
    "    \n",
    "    Parameters:\n",
    "    - reddit: PRAW Reddit instance\n",
    "    - subreddit_name: Name of the subreddit to scrape\n",
    "    - search_keywords: List of keywords to search for\n",
    "    - post_limit: Maximum number of posts to scrape per keyword\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing posts and comments data\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "    \n",
    "    for keyword in search_keywords:\n",
    "        try:\n",
    "            # Search for posts containing the keyword\n",
    "            for submission in tqdm(subreddit.search(keyword, limit=post_limit)):\n",
    "                # Extract post data\n",
    "                post_data = {\n",
    "                    'post_id': submission.id,\n",
    "                    'title': submission.title,\n",
    "                    'text': submission.selftext,\n",
    "                    'author': str(submission.author),\n",
    "                    'score': submission.score,\n",
    "                    'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
    "                    'num_comments': submission.num_comments,\n",
    "                    'upvote_ratio': submission.upvote_ratio,\n",
    "                    'subreddit': subreddit_name,\n",
    "                    'keyword': keyword,\n",
    "                    'url': f\"https://reddit.com{submission.permalink}\"\n",
    "                }\n",
    "                posts_data.append(post_data)\n",
    "                \n",
    "                # Get comments\n",
    "                submission.comments.replace_more(limit=0)  # Flatten comment tree\n",
    "                for comment in submission.comments.list():\n",
    "                    comment_data = {\n",
    "                        'comment_id': comment.id,\n",
    "                        'post_id': submission.id,\n",
    "                        'text': comment.body,\n",
    "                        'author': str(comment.author),\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'keyword': keyword\n",
    "                    }\n",
    "                    comments_data.append(comment_data)\n",
    "                \n",
    "                # Respect Reddit's API rate limits\n",
    "                time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping keyword '{keyword}' in r/{subreddit_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'posts': pd.DataFrame(posts_data),\n",
    "        'comments': pd.DataFrame(comments_data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [\n",
    "        'ChronicIllness',\n",
    "        'ClinicalTrials',\n",
    "        'Cancer',\n",
    "        'AutoimmuneProtocol',\n",
    "        'MultipleSclerosis'\n",
    "    ]\n",
    "    \n",
    "    # Keywords related to clinical trials\n",
    "keywords = [\n",
    "        'clinical trial',\n",
    "        'medical study',\n",
    "        'research study',\n",
    "        'clinical research',\n",
    "        'experimental treatment',\n",
    "        'study participant',\n",
    "        'medical research'\n",
    "    ]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, output_dir='scraped_data'):\n",
    "    \"\"\"Save scraped data to CSV files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    for data_type, df in data.items():\n",
    "        filename = f\"{output_dir}/{data_type}_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df)} {data_type} to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping r/ChronicIllness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:25,  2.52s/it]\n",
      "10it [00:27,  2.70s/it]\n",
      "10it [00:25,  2.50s/it]\n",
      "10it [00:24,  2.45s/it]\n",
      "8it [00:19,  2.45s/it]\n",
      "10it [00:24,  2.42s/it]\n",
      "10it [00:28,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 68 posts to scraped_data/posts_20241101_191655.csv\n",
      "Saved 1695 comments to scraped_data/comments_20241101_191655.csv\n",
      "\n",
      "Scraping r/ClinicalTrials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:24,  2.46s/it]\n",
      "10it [00:23,  2.39s/it]\n",
      "10it [00:23,  2.35s/it]\n",
      "10it [00:24,  2.45s/it]\n",
      "10it [00:23,  2.36s/it]\n",
      "10it [00:24,  2.41s/it]\n",
      "10it [00:24,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 138 posts to scraped_data/posts_20241101_191948.csv\n",
      "Saved 1737 comments to scraped_data/comments_20241101_191948.csv\n",
      "\n",
      "Scraping r/Cancer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:25,  2.51s/it]\n",
      "10it [00:24,  2.48s/it]\n",
      "10it [00:23,  2.38s/it]\n",
      "10it [00:25,  2.52s/it]\n",
      "10it [00:24,  2.47s/it]\n",
      "10it [00:24,  2.49s/it]\n",
      "10it [00:25,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 208 posts to scraped_data/posts_20241101_192247.csv\n",
      "Saved 2731 comments to scraped_data/comments_20241101_192247.csv\n",
      "\n",
      "Scraping r/AutoimmuneProtocol...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.73s/it]\n",
      "6it [00:14,  2.43s/it]\n",
      "10it [00:26,  2.66s/it]\n",
      "2it [00:04,  2.49s/it]\n",
      "0it [00:00, ?it/s]\n",
      "3it [00:07,  2.50s/it]\n",
      "10it [00:24,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 241 posts to scraped_data/posts_20241101_192415.csv\n",
      "Saved 3112 comments to scraped_data/comments_20241101_192415.csv\n",
      "\n",
      "Scraping r/MultipleSclerosis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:24,  2.45s/it]\n",
      "10it [00:25,  2.54s/it]\n",
      "10it [00:24,  2.46s/it]\n",
      "10it [00:24,  2.50s/it]\n",
      "10it [00:26,  2.61s/it]\n",
      "10it [00:24,  2.48s/it]\n",
      "10it [00:27,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 311 posts to scraped_data/posts_20241101_192718.csv\n",
      "Saved 4811 comments to scraped_data/comments_20241101_192718.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reddit = initialize_reddit()\n",
    "\n",
    "all_posts = []\n",
    "all_comments = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"\\nScraping r/{subreddit}...\")\n",
    "    data = scrape_subreddit(reddit, subreddit, keywords)\n",
    "        \n",
    "    all_posts.append(data['posts'])\n",
    "    all_comments.append(data['comments'])\n",
    "        \n",
    "    # Respect Reddit's API rate limits between subreddits\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = {\n",
    "        'posts': pd.concat(all_posts, ignore_index=True),\n",
    "        'comments': pd.concat(all_comments, ignore_index=True)\n",
    "    }\n",
    "    \n",
    "    # Save the scraped data\n",
    "    save_data(combined_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
